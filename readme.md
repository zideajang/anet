## 深度学习框架

### 相关分享 
- 关于 softmax 求导和 log softmax 可以在 tuts/softmax.md 找到

### todo
- 整理代码
- 更新 ReadMe
- 单元测试
- 实现优化器 Adam 和 SGD 
- 完善单元测试
- 卷积操作实现
- 移除 Context ，将其功能融入到 Functions
- 卷积前向传播的单元测试
- 卷积反向传播

### 相关资源
#### 相关说明文档

[自己动手写一个神经网络(1)—Anet 诞生记](https://juejin.cn/post/7148771409177608199/)
[深入浅出 Pytorch 系列 — 优化器的选择(2) Adagrad 、MSProp 和 adam](https://juejin.cn/post/7130601449381658631)
#### 代码
#### 视频资源
- 西瓜视频
https://www.ixigua.com/7148686902302868004?id=7147230473582969381
- 哔哩哔哩
https://space.bilibili.com/476895565?spm_id_from=333.1007.0.0


<img src="./images/backward.png">
# 从头实现深度学习框架
## 目的
参考 pytorch 以及 tinygrad 来实现一个深度学习框架，深入理解深度学习框架背景知识
## 概述
### 架构设计

### 课程目录
- 关于《softmax 求导和 log softmax》 可以在 [softmax](./tuts/softmax.md) 找到更多详细内容



### todo
- 整理代码
- 更新 ReadMe
- 单元测试
- 实现优化器 Adam 和 SGD 
- 完善单元测试
- 卷积操作实现
- 移除 Context ，将其功能融入到 Functions
- 卷积前向传播的单元测试
- 卷积反向传播

### 相关资源
#### 相关 blog 分享

[自己动手写一个神经网络(1)—Anet 诞生记](https://juejin.cn/post/7148771409177608199/)
[深入浅出 Pytorch 系列 — 优化器的选择(2) Adagrad 、MSProp 和 adam](https://juejin.cn/post/7130601449381658631)
#### 代码
#### 视频资源
- 西瓜视频
https://www.ixigua.com/7148686902302868004?id=7147230473582969381
- 哔哩哔哩
https://space.bilibili.com/476895565?spm_id_from=333.1007.0.0


<img src="./images/backward.png">